{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision # здесь находятся различные популярные датасеты\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "    torch.cuda.manual_seed(0)\n",
    "else:\n",
    "    print(\"No CUDA for you :(\")\n",
    "    \n",
    "print(torch.__version__)\n",
    "%config InlineBackend.figure_format ='retina' # чтобы графики были красивее\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Зачем нужен pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Замена ```numpy```  свозможностью работы на GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Низкоуровневая библиотека для ```DEEP LEARNING```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным элементом ```pytorch``` являются __тензоры__ - мы будем воспринимать их как н-мерные массивы.<br> \n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создавать тензоры можно и вручную из списков и из массивов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.tensor(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно тензоры обратно переводить в массивы ```numpy``` за константное время:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10,10)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(100,100)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(1000,1000)\n",
    "%timeit x.numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно так же, как и в ```numpy```, можно производить арифметические операции над тензорами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5,3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x+y)\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x*y)\n",
    "print(torch.mul(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции, заканчивающиеся на нижнее подчеркивание, __не чистые__, то есть они изменяют элемент, к которому были применены. <br> Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` поддерживаются вычисления на видеокарте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          \n",
    "    y = torch.ones_like(x, device=device)  \n",
    "    x = x.to(device)                       \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))     \n",
    "else:\n",
    "    print(\"FeelsBadMan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет ```autograd``` - одна из самых важных вещей в ```pytorch```. В нем находятся функции для автоматического взятия производных от тензоров. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У ```torch.tensor``` есть аттрибут ```.requires_grad``` , и если его поставить ```True``` , то все операции над этим тензором начнут записываться, чтобы можно было взять производную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить отслеживание операций, к тензору надо применить метод ```.detach()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один важное понятие  для ```autograd``` - это ```.grad_fn```<br>\n",
    "Это аттрибут тензора, который показывает, с помощью какой функции был создан этот тензор.<br> Если тензор был создан вручную, то ```.grad_fn = None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать производную тензора, к нему применяется метод ```.baсkward()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если тензор состоял из единственного числа, то в ```.backward``` никаких аргументов передавать не нужно. <br>\n",
    "[Подробнее об аргументах ```.backward``` - второй ответ](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим тензор с ```.requires_grad = True``` для отслеживания вычислений над ним, и совершим несколько операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 3 * y**2\n",
    "z.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение произошедшему:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{out} = \\frac14 \\sum_i{z_i} = \\frac14 \\sum_i{3~(x_i+2)^2} \\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} = \\frac{\\partial ~out}{\\partial ~z} \\frac{\\partial ~z}{\\partial ~y} \\frac{\\partial ~y}{\\partial ~x_i} \n",
    "\\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} =  \\frac32~(x_i+2) = \\frac92 = 4.5\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Задание</b>\n",
    "\n",
    "```x = torch.eye(2, requires_grad = True)\n",
    "y = x/12\n",
    "z = (y + 5)**2\n",
    "out = z.sum()```\n",
    "<br>\n",
    "<br>\n",
    "Что вернет ```x.grad``` после вызова ```out.backward()```?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель нейронной сети, в которой мы сами реализуем разбиение данных на мини-батчи, итерации по ним и прямое распространение(```forward propagation```). <br>\n",
    "Обратное распространение (```backpropagation```) сделаем при помощи ```.backward```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import make_moons # генератор данных \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym = make_moons(n_samples = 10000, noise=0.1)\n",
    "ym = ym.reshape(-1,1)\n",
    "xm_train, xm_test, ym_train, ym_test = [torch.tensor(i) for i in train_test_split(xm,ym, test_size = 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор минибатчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(inputs, targets, batchsize = 16, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_id in range(0, inputs.shape[0], batchsize):\n",
    "        end_id = min(start_id + batchsize, inputs.shape[0])\n",
    "        if shuffle:\n",
    "            batch_indices = indices[start_id:end_id]\n",
    "        else:\n",
    "            batch_indices = slice(start_id, end_id)\n",
    "        yield inputs[batch_indices], targets[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` модель состоит из слоев, например ```nn.Linear(...)```<br>\n",
    "```nn.Linear(...)``` - набор параметров (матрица весов и вектор смещения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nn.Linear(2,20).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы задать модель нейронной сети, надо определить методы ```__init__```, в котором определяется структура нейронной сети, и ```forward```, где описывается прямое распространение. <br>\n",
    "А также модель должна наследовать класс ```nn.Module```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, seed = 0):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.l1 = nn.Linear(2, 20)\n",
    "        self.l2 = nn.Linear(20, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` представлены несколько видов функций потерь, например: <br>\n",
    "```nn.MSELoss()``` - Mean Squared Error <br>\n",
    "```nn.L1Loss()``` - MAE <br>\n",
    "```nn.CrossEntropyLoss()``` - перекрестная энтропия, в которую входит softmax<br>\n",
    "То есть, если мы хотим использовать ```nn.CrossEntropyLoss()``` в качестве функции потерь в модели нейронной сети, на последнем слое __не надо применять softmax__. Также особенностью реализации ```nn.CrossEntropyLoss()``` в ```pytorch``` является то, что эта функция потерь принимает на вход __не one-hot-encoded вектора__, а обычные номера классов.\n",
    "    \n",
    "[Полный список функций потерь](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере мы взяли на себя итерирование по эпохам, минибатчам, подсчет функционала качества, обновление значений весов и смещений и обнуление градиентов тензоров после обновления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем обнулять градиенты?\n",
    "Каждый раз, когда вызывается метод ```.backward``` градиенты тензоров не обновляются (то есть их __не замещают новые значения__), а __накапливаются__. Поэтому, чтобы градиенты не смешивались, необходимо на каждом шаге их обнулять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) # это нужно, так как генератор минибатчей использует np.random.shuffle\n",
    "\n",
    "\n",
    "net1 = Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net1(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "    \n",
    "        #Обновление весов и обнуление градиентов\n",
    "        with torch.no_grad(): # torch.no_grad() отключает отслеживание вычислений над тензорами в блоке\n",
    "            for parameter in net1.parameters():\n",
    "                parameter.data -= lr * parameter.grad\n",
    "                parameter.grad = torch.zeros_like(parameter)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Того же самого результата можно добиться, используя встроенные в ```pytorch``` оптимизаторы и функции для обновления и обнуления градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виды возможных оптимизаторов:<br>\n",
    "```optim.SGD``` - стохастический градиентный градиентный спуск (на самом деле, ему можно подавать батч любого размера, так что SGD - градиентный спуск по минибатчам)<br>\n",
    "```optim.Adam```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm)<br>\n",
    "```optim.RMSprop```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop) <br>\n",
    "<br>\n",
    "[Полный список оптимизаторов](https://pytorch.org/docs/stable/optim.html])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы инициализировать оптимизатор, в него надо передать параметры обучаемой модели ```net.parameters()``` и начальную скорость обучения (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net2 = Net()\n",
    "\n",
    "# В нашем случае будем использовать обычный градиентный спуск по минибатчам, как и в прошлый раз\n",
    "optimizer = optim.SGD(net2.parameters(), lr = 0.1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net2(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(data,target):\n",
    "    df = pd.DataFrame(dict(x=data[:,0], y=data[:,1], label=target))\n",
    "    colors = {0:'red', 1:'blue'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_labels(xm_test.numpy(),np.round(net2(xm_test.float()).detach()).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление медианной стоимости дома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и раньше стоит задача предсказания медианной стоимости дома.\n",
    "Подгрузим данные и разобьем их на __train__ и __val__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "california_housing_dataframe = california_housing_dataframe.reindex(\n",
    "    np.random.permutation(california_housing_dataframe.index))\n",
    "california_housing_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "    Args:\n",
    "      california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "        from the California housing data set.\n",
    "    Returns:\n",
    "      A DataFrame that contains the features to be used for the model, including\n",
    "      synthetic features.\n",
    "    \"\"\"\n",
    "    selected_features = df[\n",
    "            [\"latitude\",\n",
    "             \"longitude\",\n",
    "             \"housing_median_age\",\n",
    "             \"total_rooms\",\n",
    "             \"total_bedrooms\",\n",
    "             \"population\",\n",
    "             \"households\",\n",
    "             \"median_income\"]]\n",
    "    processed_features = selected_features.copy()\n",
    "    # Create a synthetic feature.\n",
    "    processed_features[\"rooms_per_person\"] = (df[\"total_rooms\"]/df[\"population\"])\n",
    "    return processed_features\n",
    "\n",
    "def preprocess_targets(df):\n",
    "    \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "      Args:\n",
    "        california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "          from the California housing data set.\n",
    "      Returns:\n",
    "        A DataFrame that contains the target feature.\n",
    "      \"\"\"\n",
    "    output_targets = pd.DataFrame()\n",
    "    # Scale the target to be in units of thousands of dollars.\n",
    "    output_targets[\"median_house_value\"] = (df[\"median_house_value\"] / 1000.0)\n",
    "    return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "training_examples = preprocess_features(california_housing_dataframe.head(12000))\n",
    "training_targets = preprocess_targets(california_housing_dataframe.head(12000))\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(california_housing_dataframe.tail(5000))\n",
    "validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем все в ```torch.tensor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = torch.tensor(training_examples.values).float()\n",
    "training_targets = torch.tensor(training_targets.values).float()\n",
    "\n",
    "validation_examples = torch.tensor(validation_examples.values).float()\n",
    "validation_targets = torch.tensor(validation_targets.values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо предсказать единственное вещественное значение, а количество признаков - 9, а значит на входном слое сети будет 9 нейронов, а на выходном 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим свою сеть со скрытыми слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseValueNet(nn.Module):\n",
    "\n",
    "    def __init__(self, seed = 0):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.l1 = nn.Linear(9, 20)\n",
    "        self.l2 = nn.Linear(20, 10)\n",
    "        self.l3 = nn.Linear(10, 1)\n",
    "        with torch.no_grad(): # torch.no_grad() отключает отслеживание вычислений над тензорами в блоке\n",
    "            for parameter in self.parameters():\n",
    "                parameter.data*=0.01\n",
    "          \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = HouseValueNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть. В качестве оптимизатора возьмем ```optim.Adam``` с ```lr = 0.05```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net3 = HouseValueNet()\n",
    "\n",
    "# Будем использовать обычный градиентный спуск по минибатчам, как и в прошлый раз\n",
    "optimizer = optim.Adam(net3.parameters(),  lr = 0.05)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    batch_counter = 0\n",
    "    for batch in generate_minibatches(training_examples,training_targets,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net3(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = torch.sqrt(criterion(output, labels.float()))\n",
    "\n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        losses.append(loss)\n",
    "        batch_counter +=1\n",
    "        \n",
    "    if epoch%10==0:\n",
    "        print(f\"Mean loss on epoch {epoch} - {running_loss/batch_counter}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим качество полученной модели на отложенной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(criterion(net3(validation_examples), validation_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть на датасете  MNIST, содержащем 10 видов различной одежды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "ycat_train = to_categorical(y_train, num_classes=10)\n",
    "ycat_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycat_train = torch.tensor(ycat_train)\n",
    "ycat_test = torch.tensor(ycat_test)\n",
    "\n",
    "x_train = torch.tensor(x_train).view(-1,28*28).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "x_test = torch.tensor(x_test).view(-1,28*28).float()\n",
    "y_test = torch.tensor(y_test).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель с ```nn.CrossEntropyLoss()```. На последнем слое не будем применять ```torch.softmax```, так как в ```nn.CrossEntropyLoss()``` эта функция уже применяется. <br>\n",
    "Но чтобы получить нормальные предсказания в виде вектора вероятностей быть той или иной цифрой и проверить точность предсказаний, применить ```torch.softmax``` все же придется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistSoftmaxNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x.float()))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MnistSoftmaxNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net = MnistSoftmaxNet()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    batch_counter = 0\n",
    "    for batch in generate_minibatches(x_train, y_train, 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        batch_counter+=1\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f\"Mean loss on epoch {epoch} - {running_loss/batch_counter}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили довольно высокую точность на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float()),-1),dim = -1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, создавать модели можно и более простым способом. Но тогда теряется гибкость настройки модели и мы лишаемся возможности создавать свои слои:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(784, 128))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(128, 64))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(64, 10))\n",
    "\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    batch_counter = 0\n",
    "    for batch in generate_minibatches(x_train, y_train, 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss #вызов .item() переводит полученное число на cpu, что замедляет работу\n",
    "        \n",
    "        batch_counter+=1\n",
    "        \n",
    "    if epoch%1==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss/batch_counter}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float()),-1),dim = -1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели базовые понятия ```pytorch``` и научились строить модели полносвязных нейронных сетей используя встроенные функции."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
