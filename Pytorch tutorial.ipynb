{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n",
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision # здесь находятся различные популярные датасеты\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "    torch.cuda.manual_seed(0)\n",
    "else:\n",
    "    print(\"No CUDA for you :(\")\n",
    "    \n",
    "print(torch.__version__)\n",
    "%config InlineBackend.figure_format ='retina' # чтобы графики были красивее\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Зачем нужен pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Замена ```numpy```  свозможностью работы на GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Низкоуровневая библиотека для ```DEEP LEARNING```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным элементом ```pytorch``` являются __тензоры__ - мы будем воспринимать их как н-мерные массивы.<br> \n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9101, 0.4188, 0.2682, 0.2987],\n",
       "        [0.5180, 0.2416, 0.9405, 0.0702],\n",
       "        [0.3869, 0.3611, 0.0395, 0.0444],\n",
       "        [0.5650, 0.0112, 0.4219, 0.0568],\n",
       "        [0.3878, 0.4993, 0.1972, 0.1816]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создавать тензоры можно и вручную из списков и из массивов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.tensor(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно тензоры обратно переводить в массивы ```numpy``` за константное время:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.4 ns ± 1.82 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
      "43.3 ns ± 1.15 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
      "42.8 ns ± 1.64 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10,10)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(100,100)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(1000,1000)\n",
    "%timeit x.numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно так же, как и в ```numpy```, можно производить арифметические операции над тензорами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5,3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.9148, 0.5285, 0.6135],\n",
      "        [0.0910, 0.2965, 0.0315],\n",
      "        [0.0187, 0.6073, 0.8055],\n",
      "        [0.0524, 0.6035, 0.0723],\n",
      "        [0.6651, 0.3472, 0.3532]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9148, 1.5285, 1.6135],\n",
      "        [1.0910, 1.2965, 1.0315],\n",
      "        [1.0187, 1.6073, 1.8055],\n",
      "        [1.0524, 1.6035, 1.0723],\n",
      "        [1.6651, 1.3472, 1.3532]])\n",
      "tensor([[1.9148, 1.5285, 1.6135],\n",
      "        [1.0910, 1.2965, 1.0315],\n",
      "        [1.0187, 1.6073, 1.8055],\n",
      "        [1.0524, 1.6035, 1.0723],\n",
      "        [1.6651, 1.3472, 1.3532]])\n"
     ]
    }
   ],
   "source": [
    "print(x+y)\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9148, 0.5285, 0.6135],\n",
      "        [0.0910, 0.2965, 0.0315],\n",
      "        [0.0187, 0.6073, 0.8055],\n",
      "        [0.0524, 0.6035, 0.0723],\n",
      "        [0.6651, 0.3472, 0.3532]])\n",
      "tensor([[0.9148, 0.5285, 0.6135],\n",
      "        [0.0910, 0.2965, 0.0315],\n",
      "        [0.0187, 0.6073, 0.8055],\n",
      "        [0.0524, 0.6035, 0.0723],\n",
      "        [0.6651, 0.3472, 0.3532]])\n"
     ]
    }
   ],
   "source": [
    "print(x*y)\n",
    "print(torch.mul(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции, заканчивающиеся на нижнее подчеркивание, __не чистые__, то есть они изменяют элемент, к которому были применены. <br> Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9148, 0.5285, 0.6135],\n",
      "        [0.0910, 0.2965, 0.0315],\n",
      "        [0.0187, 0.6073, 0.8055],\n",
      "        [0.0524, 0.6035, 0.0723],\n",
      "        [0.6651, 0.3472, 0.3532]])\n",
      "tensor([[1.9148, 1.5285, 1.6135],\n",
      "        [1.0910, 1.2965, 1.0315],\n",
      "        [1.0187, 1.6073, 1.8055],\n",
      "        [1.0524, 1.6035, 1.0723],\n",
      "        [1.6651, 1.3472, 1.3532]])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` поддерживаются вычисления на видеокарте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          \n",
    "    y = torch.ones_like(x, device=device)  \n",
    "    x = x.to(device)                       \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))     \n",
    "else:\n",
    "    print(\"FeelsBadMan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет ```autograd``` - одна из самых важных вещей в ```pytorch```. В нем находятся функции для автоматического взятия производных от тензоров. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У ```torch.tensor``` есть аттрибут ```.requires_grad``` , и если его поставить ```True``` , то все операции над этим тензором начнут записываться, чтобы можно было взять производную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить отслеживание операций, к тензору надо применить метод ```.detach()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один важное понятие  для ```autograd``` - это ```.grad_fn```<br>\n",
    "Это аттрибут тензора, который показывает, с помощью какой функции был создан этот тензор.<br> Если тензор был создан вручную, то ```.grad_fn = None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f1df339d4a8>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать производную тензора, к нему применяется метод ```.baсkward()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если тензор состоял из единственного числа, то в ```.backward``` никаких аргументов передавать не нужно. <br>\n",
    "[Подробнее об аргументах ```.backward``` - второй ответ](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим тензор с ```.requires_grad = True``` для отслеживания вычислений над ним, и совершим несколько операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27., 27.],\n",
       "        [27., 27.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 3 * y**2\n",
    "z.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение произошедшему:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{out} = \\frac14 \\sum_i{z_i} = \\frac14 \\sum_i{3~(x_i+2)^2} \\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} = \\frac{\\partial ~out}{\\partial ~z} \\frac{\\partial ~z}{\\partial ~y} \\frac{\\partial ~y}{\\partial ~x_i} \n",
    "\\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} =  \\frac32~(x_i+2) = \\frac92 = 4.5\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Задание</b>\n",
    "\n",
    "```x = torch.eye(2, requires_grad = True)\n",
    "y = x/12\n",
    "z = (y + 5)**2\n",
    "out = z.sum()```\n",
    "<br>\n",
    "<br>\n",
    "Что вернет ```x.grad``` после вызова ```out.backward()```?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель нейронной сети, в которой мы сами реализуем разбиение данных на мини-батчи, итерации по ним и прямое распространение(```forward propagation```). <br>\n",
    "Обратное распространение (```backpropagation```) сделаем при помощи ```.backward```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import make_moons # генератор данных \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym = make_moons(n_samples = 10000, noise=0.1)\n",
    "ym = ym.reshape(-1,1)\n",
    "xm_train, xm_test, ym_train, ym_test = [torch.tensor(i) for i in train_test_split(xm,ym, test_size = 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор минибатчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(inputs, targets, batchsize = 16, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_id in range(0, inputs.shape[0], batchsize):\n",
    "        end_id = min(start_id + batchsize, inputs.shape[0])\n",
    "        if shuffle:\n",
    "            batch_indices = indices[start_id:end_id]\n",
    "        else:\n",
    "            batch_indices = slice(start_id, end_id)\n",
    "        yield inputs[batch_indices], targets[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` модель состоит из слоев, например ```nn.Linear(...)```<br>\n",
    "```nn.Linear(...)``` - набор параметров (матрица весов и вектор смещения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.6599,  0.1778],\n",
       "         [-0.6901,  0.4444],\n",
       "         [ 0.3708, -0.4682],\n",
       "         [ 0.0982, -0.1595],\n",
       "         [ 0.1528,  0.5754],\n",
       "         [ 0.1753,  0.4468],\n",
       "         [ 0.4568, -0.5449],\n",
       "         [ 0.1226,  0.5015],\n",
       "         [ 0.3521, -0.3648],\n",
       "         [-0.1962, -0.3849],\n",
       "         [-0.1517, -0.6633],\n",
       "         [ 0.0438,  0.5590],\n",
       "         [ 0.5617, -0.1383],\n",
       "         [-0.2998, -0.2041],\n",
       "         [-0.6453, -0.1334],\n",
       "         [-0.2824, -0.2568],\n",
       "         [ 0.3037, -0.1958],\n",
       "         [-0.2940, -0.5335],\n",
       "         [ 0.5202,  0.2177],\n",
       "         [ 0.4755,  0.0627]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.5208,  0.1407,  0.1354,  0.0997,  0.4198,  0.5355,  0.4634,  0.2314,\n",
       "         -0.3067,  0.4205,  0.2872,  0.3931, -0.4119, -0.6050, -0.7064,  0.6083,\n",
       "         -0.3079,  0.2361, -0.4718, -0.4321], requires_grad=True)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nn.Linear(2,20).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы задать модель нейронной сети, надо определить методы ```__init__```, в котором определяется структура нейронной сети, и ```forward```, где описывается прямое распространение. <br>\n",
    "А также модель должна наследовать класс ```nn.Module```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=2, out_features=20, bias=True)\n",
      "  (l2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, seed = 0):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.l1 = nn.Linear(2, 20)\n",
    "        self.l2 = nn.Linear(20, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` представлены несколько видов функций потерь, например: <br>\n",
    "```nn.MSELoss()``` - Mean Squared Error <br>\n",
    "```nn.L1Loss()``` - MAE <br>\n",
    "```nn.CrossEntropyLoss()``` - перекрестная энтропия, в которую входит softmax<br>\n",
    "То есть, если мы хотим использовать ```nn.CrossEntropyLoss()``` в качестве функции потерь в модели нейронной сети, на последнем слое __не надо применять softmax__. Также особенностью реализации ```nn.CrossEntropyLoss()``` в ```pytorch``` является то, что эта функция потерь принимает на вход __не one-hot-encoded вектора__, а обычные номера классов.\n",
    "    \n",
    "[Полный список функций потерь](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере мы взяли на себя итерирование по эпохам, минибатчам, подсчет функционала качества, обновление значений весов и смещений и обнуление градиентов тензоров после обновления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем обнулять градиенты?\n",
    "Каждый раз, когда вызывается метод ```.backward``` градиенты тензоров не обновляются (то есть их __не замещают новые значения__), а __накапливаются__. Поэтому, чтобы градиенты не смешивались, необходимо на каждом шаге их обнулять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 19.97942603379488\n",
      "Loss on epoch 10 - 9.293201211839914\n",
      "Loss on epoch 20 - 7.114689467474818\n",
      "Loss on epoch 30 - 4.072639557532966\n",
      "Loss on epoch 40 - 2.2350261071696877\n",
      "Loss on epoch 50 - 1.4497888488695025\n",
      "Loss on epoch 60 - 1.0624717893078923\n",
      "Loss on epoch 70 - 0.8400313325691968\n",
      "Loss on epoch 80 - 0.6973414598032832\n",
      "Loss on epoch 90 - 0.5985110647743568\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) # это нужно, так как генератор минибатчей использует np.random.shuffle\n",
    "\n",
    "\n",
    "net1 = Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net1(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "    \n",
    "        #Обновление весов и обнуление градиентов\n",
    "        with torch.no_grad(): # torch.no_grad() отключает отслеживание вычислений над тензорами в блоке\n",
    "            for parameter in net1.parameters():\n",
    "                parameter.data -= lr * parameter.grad\n",
    "                parameter.grad = torch.zeros_like(parameter)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Того же самого результата можно добиться, используя встроенные в ```pytorch``` оптимизаторы и функции для обновления и обнуления градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виды возможных оптимизаторов:<br>\n",
    "```optim.SGD``` - стохастический градиентный градиентный спуск (на самом деле, ему можно подавать батч любого размера, так что SGD - градиентный спуск по минибатчам)<br>\n",
    "```optim.Adam```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm)<br>\n",
    "```optim.RMSprop```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop) <br>\n",
    "<br>\n",
    "[Полный список оптимизаторов](https://pytorch.org/docs/stable/optim.html])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы инициализировать оптимизатор, в него надо передать параметры обучаемой модели ```net.parameters()``` и начальную скорость обучения (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 19.979425854980946\n",
      "Loss on epoch 10 - 9.29320128262043\n",
      "Loss on epoch 20 - 7.114690449088812\n",
      "Loss on epoch 30 - 4.072639727033675\n",
      "Loss on epoch 40 - 2.2350257812067866\n",
      "Loss on epoch 50 - 1.4497885359451175\n",
      "Loss on epoch 60 - 1.0624716572929174\n",
      "Loss on epoch 70 - 0.8400311646983027\n",
      "Loss on epoch 80 - 0.6973412396619096\n",
      "Loss on epoch 90 - 0.5985110595356673\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net2 = Net()\n",
    "\n",
    "# В нашем случае будем использовать обычный градиентный спуск по минибатчам, как и в прошлый раз\n",
    "optimizer = optim.SGD(net2.parameters(), lr = 0.1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net2(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(data,target):\n",
    "    df = pd.DataFrame(dict(x=data[:,0], y=data[:,1], label=target))\n",
    "    colors = {0:'red', 1:'blue'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_labels(xm_test.numpy(),np.round(net2(xm_test.float()).detach()).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть на датасете Fashion MNIST, содержащем 10 видов различной одежды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "ycat_train = to_categorical(y_train, num_classes=10)\n",
    "ycat_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1df2afc358>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADgpJREFUeJzt3X+MVfWZx/HPs1j+kKI4aQRCYSnEYJW4082IjSWrxkzVDQZHrekkJjQapn8wiU02ZA3/VNNgyCrslmiamaZYSFpKE3VB0iw0otLGZuKIWC0srTFsO3IDNTjywx9kmGf/mEMzxbnfe+fec++5zPN+JeT+eM6558kNnznn3O+592vuLgDx/EPRDQAoBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUZc3cmJlxOSHQYO5u1SxX157fzO40syNm9q6ZPVrPawFoLqv12n4zmybpj5I6JQ1Jel1St7sfSqzDnh9osGbs+ZdJetfd33P3c5J+IWllHa8HoInqCf88SX8Z93goe+7vmFmPmQ2a2WAd2wKQs3o+8Jvo0OJzh/Xu3i+pX+KwH2gl9ez5hyTNH/f4y5KO1dcOgGapJ/yvS7rGzL5iZtMlfVvSrnzaAtBoNR/2u/uImfVK2iNpmqQt7v6H3DoD0FA1D/XVtDHO+YGGa8pFPgAuXYQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfMU3ZJkZkclnZZ0XtKIu3fk0RTyM23atGT9yiuvbOj2e3t7y9Yuv/zy5LpLlixJ1tesWZOsP/XUU2Vr3d3dyXU//fTTZH3Dhg3J+uOPP56st4K6wp+5zd0/yOF1ADQRh/1AUPWG3yXtNbM3zKwnj4YANEe9h/3fcPdjZna1pF+b2f+6+/7xC2R/FPjDALSYuvb87n4suz0h6QVJyyZYpt/dO/gwEGgtNYffzGaY2cwL9yV9U9I7eTUGoLHqOeyfLekFM7vwOj939//JpSsADVdz+N39PUn/lGMvU9aCBQuS9enTpyfrN998c7K+fPnysrVZs2Yl173vvvuS9SINDQ0l65s3b07Wu7q6ytZOnz6dXPett95K1l999dVk/VLAUB8QFOEHgiL8QFCEHwiK8ANBEX4gKHP35m3MrHkba6L29vZkfd++fcl6o79W26pGR0eT9YceeihZP3PmTM3bLpVKyfqHH36YrB85cqTmbTeau1s1y7HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfPQVtbW7I+MDCQrC9atCjPdnJVqffh4eFk/bbbbitbO3fuXHLdqNc/1ItxfgBJhB8IivADQRF+ICjCDwRF+IGgCD8QVB6z9IZ38uTJZH3t2rXJ+ooVK5L1N998M1mv9BPWKQcPHkzWOzs7k/WzZ88m69dff33Z2iOPPJJcF43Fnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX4z2yJphaQT7r40e65N0g5JCyUdlfSAu6d/6FxT9/v89briiiuS9UrTSff19ZWtPfzww8l1H3zwwWR9+/btyTpaT57f5/+ppDsveu5RSS+5+zWSXsoeA7iEVAy/u++XdPElbCslbc3ub5V0T859AWiwWs/5Z7t7SZKy26vzawlAMzT82n4z65HU0+jtAJicWvf8x81sriRltyfKLeju/e7e4e4dNW4LQAPUGv5dklZl91dJ2plPOwCapWL4zWy7pN9JWmJmQ2b2sKQNkjrN7E+SOrPHAC4hFc/53b27TOn2nHsJ69SpU3Wt/9FHH9W87urVq5P1HTt2JOujo6M1bxvF4go/ICjCDwRF+IGgCD8QFOEHgiL8QFBM0T0FzJgxo2ztxRdfTK57yy23JOt33XVXsr53795kHc3HFN0Akgg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ae4xYsXJ+sHDhxI1oeHh5P1l19+OVkfHBwsW3vmmWeS6zbz/+ZUwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7gurq6kvVnn302WZ85c2bN2163bl2yvm3btmS9VCrVvO2pjHF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxXF+M9siaYWkE+6+NHvuMUmrJf01W2ydu/+q4sYY57/kLF26NFnftGlTsn777bXP5N7X15esr1+/Pll///33a972pSzPcf6fSrpzguf/093bs38Vgw+gtVQMv7vvl3SyCb0AaKJ6zvl7zez3ZrbFzK7KrSMATVFr+H8kabGkdkklSRvLLWhmPWY2aGblf8wNQNPVFH53P+7u5919VNKPJS1LLNvv7h3u3lFrkwDyV1P4zWzuuIddkt7Jpx0AzXJZpQXMbLukWyV9ycyGJH1f0q1m1i7JJR2V9N0G9gigAfg+P+oya9asZP3uu+8uW6v0WwFm6eHqffv2JeudnZ3J+lTF9/kBJBF+ICjCDwRF+IGgCD8QFOEHgmKoD4X57LPPkvXLLktfhjIyMpKs33HHHWVrr7zySnLdSxlDfQCSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqIrf50dsN9xwQ7J+//33J+s33nhj2VqlcfxKDh06lKzv37+/rtef6tjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNPcUuWLEnWe3t7k/V77703WZ8zZ86ke6rW+fPnk/VSqZSsj46O5tnOlMOeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2bzJW2TNEfSqKR+d/+hmbVJ2iFpoaSjkh5w9w8b12pclcbSu7u7y9YqjeMvXLiwlpZyMTg4mKyvX78+Wd+1a1ee7YRTzZ5/RNK/uftXJX1d0hozu07So5JecvdrJL2UPQZwiagYfncvufuB7P5pSYclzZO0UtLWbLGtku5pVJMA8jepc34zWyjpa5IGJM1295I09gdC0tV5Nwegcaq+tt/MvijpOUnfc/dTZlVNByYz65HUU1t7ABqlqj2/mX1BY8H/mbs/nz193MzmZvW5kk5MtK6797t7h7t35NEwgHxUDL+N7eJ/Iumwu28aV9olaVV2f5Wknfm3B6BRKk7RbWbLJf1G0tsaG+qTpHUaO+//paQFkv4s6VvufrLCa4Wconv27NnJ+nXXXZesP/3008n6tddeO+me8jIwMJCsP/nkk2VrO3em9xd8Jbc21U7RXfGc391/K6nci90+maYAtA6u8AOCIvxAUIQfCIrwA0ERfiAowg8ExU93V6mtra1sra+vL7lue3t7sr5o0aKaesrDa6+9lqxv3LgxWd+zZ0+y/sknn0y6JzQHe34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCrMOP9NN92UrK9duzZZX7ZsWdnavHnzauopLx9//HHZ2ubNm5PrPvHEE8n62bNna+oJrY89PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EFWacv6urq656PQ4dOpSs7969O1kfGRlJ1lPfuR8eHk6ui7jY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUObu6QXM5kvaJmmOpFFJ/e7+QzN7TNJqSX/NFl3n7r+q8FrpjQGom7tbNctVE/65kua6+wEzmynpDUn3SHpA0hl3f6rapgg/0HjVhr/iFX7uXpJUyu6fNrPDkor96RoAdZvUOb+ZLZT0NUkD2VO9ZvZ7M9tiZleVWafHzAbNbLCuTgHkquJh/98WNPuipFclrXf3581stqQPJLmkH2js1OChCq/BYT/QYLmd80uSmX1B0m5Je9x90wT1hZJ2u/vSCq9D+IEGqzb8FQ/7zcwk/UTS4fHBzz4IvKBL0juTbRJAcar5tH+5pN9IeltjQ32StE5St6R2jR32H5X03ezDwdRrsecHGizXw/68EH6g8XI77AcwNRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCavYU3R9I+r9xj7+UPdeKWrW3Vu1Lorda5dnbP1a7YFO/z/+5jZsNuntHYQ0ktGpvrdqXRG+1Kqo3DvuBoAg/EFTR4e8vePsprdpbq/Yl0VutCumt0HN+AMUpes8PoCCFhN/M7jSzI2b2rpk9WkQP5ZjZUTN728wOFj3FWDYN2gkze2fcc21m9msz+1N2O+E0aQX19piZvZ+9dwfN7F8L6m2+mb1sZofN7A9m9kj2fKHvXaKvQt63ph/2m9k0SX+U1ClpSNLrkrrd/VBTGynDzI5K6nD3wseEzexfJJ2RtO3CbEhm9h+STrr7huwP51Xu/u8t0ttjmuTMzQ3qrdzM0t9Rge9dnjNe56GIPf8ySe+6+3vufk7SLyStLKCPlufu+yWdvOjplZK2Zve3auw/T9OV6a0luHvJ3Q9k909LujCzdKHvXaKvQhQR/nmS/jLu8ZBaa8pvl7TXzN4ws56im5nA7AszI2W3Vxfcz8UqztzcTBfNLN0y710tM17nrYjwTzSbSCsNOXzD3f9Z0l2S1mSHt6jOjyQt1tg0biVJG4tsJptZ+jlJ33P3U0X2Mt4EfRXyvhUR/iFJ88c9/rKkYwX0MSF3P5bdnpD0gsZOU1rJ8QuTpGa3Jwru52/c/bi7n3f3UUk/VoHvXTaz9HOSfubuz2dPF/7eTdRXUe9bEeF/XdI1ZvYVM5su6duSdhXQx+eY2YzsgxiZ2QxJ31TrzT68S9Kq7P4qSTsL7OXvtMrMzeVmllbB712rzXhdyEU+2VDGf0maJmmLu69vehMTMLNFGtvbS2PfePx5kb2Z2XZJt2rsW1/HJX1f0n9L+qWkBZL+LOlb7t70D97K9HarJjlzc4N6Kzez9IAKfO/ynPE6l364wg+IiSv8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycat_train = torch.tensor(ycat_train)\n",
    "ycat_test = torch.tensor(ycat_test)\n",
    "\n",
    "x_train = torch.tensor(x_train).view(-1,28*28).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "x_test = torch.tensor(x_test).view(-1,28*28).float()\n",
    "y_test = torch.tensor(y_test).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000]) torch.Size([10000, 784]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель с ```nn.CrossEntropyLoss()```. На последнем слое не будем применять ```torch.softmax```, так как в ```nn.CrossEntropyLoss()``` эта функция уже применяется. <br>\n",
    "Но чтобы получить нормальные предсказания в виде вектора вероятностей быть той или иной цифрой и проверить точность предсказаний, применить ```torch.softmax``` все же придется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistSoftmaxNet(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MnistSoftmaxNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x.float()))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MnistSoftmaxNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:11,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:05<00:08,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 1 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:08<00:05,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 2 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:11<00:02,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 3 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [00:13<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 4 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net = MnistSoftmaxNet()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(x_train, y_train, 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        #running_loss += loss.item()\n",
    "            \n",
    "    if epoch%1==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили довольно высокую точность на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float()),-1),dim = -1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, создавать модели можно и более простым способом. Но тогда теряется гибкость настройки модели и мы лишаемся возможности создавать свои слои:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(784, 128))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(128, 64))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(64, 10))\n",
    "\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:09,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 325.4369812011719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:04<00:07,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 1 - 131.5320281982422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:07<00:04,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 2 - 89.64724731445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:09<00:02,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 3 - 66.59255981445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [00:12<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 4 - 51.39384078979492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(x_train, y_train, 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss #вызов .item() переводит полученное число на cpu, что замедляет работу\n",
    "    if epoch%1==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float()),-1),dim = -1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели базовые понятия ```pytorch``` и научились строить модели полносвязных нейронных сетей используя встроенные функции."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
