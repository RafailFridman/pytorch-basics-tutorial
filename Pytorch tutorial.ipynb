{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n",
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision # здесь находятся различные популярные датасеты\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "    torch.cuda.manual_seed(0)\n",
    "else:\n",
    "    print(\"No CUDA for you :(\")\n",
    "    \n",
    "print(torch.__version__)\n",
    "%config InlineBackend.figure_format ='retina' # чтобы графики были красивее\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Зачем нужен pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Замена ```numpy```  свозможностью работы на GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Низкоуровневая библиотека для ```DEEP LEARNING```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным элементом ```pytorch``` являются __тензоры__ - мы будем воспринимать их как н-мерные массивы.<br> \n",
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2429, 0.0491, 0.9640, 0.1701],\n",
       "        [0.4540, 0.0652, 0.9835, 0.2029],\n",
       "        [0.7102, 0.1049, 0.1260, 0.0643],\n",
       "        [0.1535, 0.2632, 0.9713, 0.3331],\n",
       "        [0.7554, 0.1940, 0.0049, 0.0763]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создавать тензоры можно и вручную из списков и из массивов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.tensor(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно тензоры обратно переводить в массивы ```numpy``` за константное время:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.7 ns ± 1.86 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
      "47.7 ns ± 3.04 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
      "44.8 ns ± 1.78 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10,10)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(100,100)\n",
    "%timeit x.numpy\n",
    "x = torch.rand(1000,1000)\n",
    "%timeit x.numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно так же, как и в ```numpy```, можно производить арифметические операции над тензорами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5,3)\n",
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.2321, 0.3217, 0.9892],\n",
      "        [0.9669, 0.9945, 0.0761],\n",
      "        [0.9869, 0.1623, 0.4165],\n",
      "        [0.6952, 0.0989, 0.6357],\n",
      "        [0.5219, 0.7796, 0.8784]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2321, 1.3217, 1.9892],\n",
      "        [1.9669, 1.9945, 1.0761],\n",
      "        [1.9869, 1.1623, 1.4165],\n",
      "        [1.6952, 1.0989, 1.6357],\n",
      "        [1.5219, 1.7796, 1.8784]])\n",
      "tensor([[1.2321, 1.3217, 1.9892],\n",
      "        [1.9669, 1.9945, 1.0761],\n",
      "        [1.9869, 1.1623, 1.4165],\n",
      "        [1.6952, 1.0989, 1.6357],\n",
      "        [1.5219, 1.7796, 1.8784]])\n"
     ]
    }
   ],
   "source": [
    "print(x+y)\n",
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2321, 0.3217, 0.9892],\n",
      "        [0.9669, 0.9945, 0.0761],\n",
      "        [0.9869, 0.1623, 0.4165],\n",
      "        [0.6952, 0.0989, 0.6357],\n",
      "        [0.5219, 0.7796, 0.8784]])\n",
      "tensor([[0.2321, 0.3217, 0.9892],\n",
      "        [0.9669, 0.9945, 0.0761],\n",
      "        [0.9869, 0.1623, 0.4165],\n",
      "        [0.6952, 0.0989, 0.6357],\n",
      "        [0.5219, 0.7796, 0.8784]])\n"
     ]
    }
   ],
   "source": [
    "print(x*y)\n",
    "print(torch.mul(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции, заканчивающиеся на нижнее подчеркивание, __не чистые__, то есть они изменяют элемент, к которому были применены. <br> Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2321, 0.3217, 0.9892],\n",
      "        [0.9669, 0.9945, 0.0761],\n",
      "        [0.9869, 0.1623, 0.4165],\n",
      "        [0.6952, 0.0989, 0.6357],\n",
      "        [0.5219, 0.7796, 0.8784]])\n",
      "tensor([[1.2321, 1.3217, 1.9892],\n",
      "        [1.9669, 1.9945, 1.0761],\n",
      "        [1.9869, 1.1623, 1.4165],\n",
      "        [1.6952, 1.0989, 1.6357],\n",
      "        [1.5219, 1.7796, 1.8784]])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` поддерживаются вычисления на видеокарте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          \n",
    "    y = torch.ones_like(x, device=device)  \n",
    "    x = x.to(device)                       \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))     \n",
    "else:\n",
    "    print(\"FeelsBadMan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет ```autograd``` - одна из самых важных вещей в ```pytorch```. В нем находятся функции для автоматического взятия производных от тензоров. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У ```torch.tensor``` есть аттрибут ```.requires_grad``` , и если его поставить ```True``` , то все операции над этим тензором начнут записываться, чтобы можно было взять производную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить отслеживание операций, к тензору надо применить метод ```.detach()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один важное понятие  для ```autograd``` - это ```.grad_fn```<br>\n",
    "Это аттрибут тензора, который показывает, с помощью какой функции был создан этот тензор.<br> Если тензор был создан вручную, то ```.grad_fn = None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f0c14665630>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать производную тензора, к нему применяется метод ```.baсkward()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если тензор состоял из единственного числа, то в ```.backward``` никаких аргументов передавать не нужно. <br>\n",
    "[Подробнее об аргументах ```.backward``` - второй ответ](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим тензор с ```.requires_grad = True``` для отслеживания вычислений над ним, и совершим несколько операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27., 27.],\n",
       "        [27., 27.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 3 * y**2\n",
    "z.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение произошедшему:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{out} = \\frac14 \\sum_i{z_i} = \\frac14 \\sum_i{3~(x_i+2)^2} \\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} = \\frac{\\partial ~out}{\\partial ~z} \\frac{\\partial ~z}{\\partial ~y} \\frac{\\partial ~y}{\\partial ~x_i} \n",
    "\\newline\n",
    "\\newline\n",
    "\\frac{\\partial ~\\textbf{out}}{\\partial ~x_i} =  \\frac32~(x_i+2) = \\frac92 = 4.5\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Задание</b>\n",
    "\n",
    "```x = torch.eye(2, requires_grad = True)\n",
    "y = x/12\n",
    "z = (y + 5)**2\n",
    "out = z.sum()```\n",
    "<br>\n",
    "<br>\n",
    "Что вернет ```x.grad``` после вызова ```out.backward()```?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель нейронной сети, в которой мы сами реализуем разбиение данных на мини-батчи, итерации по ним и прямое распространение(```forward propagation```). <br>\n",
    "Обратное распространение (```backpropagation```) сделаем при помощи ```.backward```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import make_moons # генератор данных \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym = make_moons(n_samples = 10000, noise=0.1)\n",
    "ym = ym.reshape(-1,1)\n",
    "xm_train, xm_test, ym_train, ym_test = [torch.tensor(i) for i in train_test_split(xm,ym, test_size = 0.2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор минибатчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(inputs, targets, batchsize = 16, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_id in range(0, inputs.shape[0], batchsize):\n",
    "        end_id = min(start_id + batchsize, inputs.shape[0])\n",
    "        if shuffle:\n",
    "            batch_indices = indices[start_id:end_id]\n",
    "        else:\n",
    "            batch_indices = slice(start_id, end_id)\n",
    "        yield inputs[batch_indices], targets[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` модель состоит из слоев, например ```nn.Linear(...)```<br>\n",
    "```nn.Linear(...)``` - набор параметров (матрица весов и вектор смещения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3384, -0.1434],\n",
       "         [ 0.4417, -0.4705],\n",
       "         [ 0.6971, -0.0622],\n",
       "         [ 0.0152, -0.6456],\n",
       "         [-0.1543,  0.0858],\n",
       "         [-0.2988, -0.6053],\n",
       "         [ 0.0841,  0.6390],\n",
       "         [ 0.0080,  0.5917],\n",
       "         [-0.0421, -0.2467],\n",
       "         [ 0.5977, -0.6971],\n",
       "         [ 0.0961, -0.5352],\n",
       "         [-0.6605,  0.1389],\n",
       "         [ 0.2459, -0.2972],\n",
       "         [ 0.6533,  0.4164],\n",
       "         [ 0.0262,  0.5639],\n",
       "         [-0.5616,  0.2540],\n",
       "         [-0.5884, -0.6602],\n",
       "         [ 0.6751,  0.3108],\n",
       "         [ 0.5011,  0.2392],\n",
       "         [ 0.3176, -0.5926]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.6308, -0.0173, -0.2542, -0.6184, -0.0598, -0.2720,  0.2251, -0.5123,\n",
       "         -0.5430,  0.2108,  0.2676,  0.2782, -0.5270,  0.1903,  0.0740,  0.1075,\n",
       "          0.2766, -0.6817, -0.6117,  0.3875], requires_grad=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nn.Linear(2,20).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы задать модель нейронной сети, надо определить методы ```__init__```, в котором определяется структура нейронной сети, и ```forward```, где описывается прямое распространение. <br>\n",
    "А также модель должна наследовать класс ```nn.Module```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=2, out_features=20, bias=True)\n",
      "  (l2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, seed = 0):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.l1 = nn.Linear(2, 20)\n",
    "        self.l2 = nn.Linear(20, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ```pytorch``` представлены несколько видов функций потерь, например: <br>\n",
    "```nn.MSELoss()``` - Mean Squared Error <br>\n",
    "```nn.L1Loss()``` - MAE <br>\n",
    "```nn.CrossEntropyLoss()``` - перекрестная энтропия, в которую входит softmax<br>\n",
    "То есть, если мы хотим использовать ```nn.CrossEntropyLoss()``` в качестве функции потерь в модели нейронной сети, на последнем слое __не надо применять softmax__. Также особенностью реализации ```nn.CrossEntropyLoss()``` в ```pytorch``` является то, что эта функция потерь принимает на вход __не one-hot-encoded вектора__, а обычные номера классов.\n",
    "    \n",
    "[Полный список функций потерь](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере мы взяли на себя итерирование по эпохам, минибатчам, подсчет функционала качества, обновление значений весов и смещений и обнуление градиентов тензоров после обновления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем обнулять градиенты?\n",
    "Каждый раз, когда вызывается метод ```.backward``` градиенты тензоров не обновляются (то есть их __не замещают новые значения__), а __накапливаются__. Поэтому, чтобы градиенты не смешивались, необходимо на каждом шаге их обнулять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 20.018548607826233\n",
      "Loss on epoch 10 - 9.338071977719665\n",
      "Loss on epoch 20 - 7.126234192401171\n",
      "Loss on epoch 30 - 4.094889936968684\n",
      "Loss on epoch 40 - 2.287942968774587\n",
      "Loss on epoch 50 - 1.5102367596700788\n",
      "Loss on epoch 60 - 1.1316262218169868\n",
      "Loss on epoch 70 - 0.9088677535764873\n",
      "Loss on epoch 80 - 0.767420397256501\n",
      "Loss on epoch 90 - 0.6678517437539995\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed) # это нужно, так как генератор минибатчей использует np.random.shuffle\n",
    "\n",
    "\n",
    "net1 = Net()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net1(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "    \n",
    "        #Обновление весов и обнуление градиентов\n",
    "        with torch.no_grad(): # torch.no_grad() отключает отслеживание вычислений над тензорами в блоке\n",
    "            for parameter in net1.parameters():\n",
    "                parameter.data -= lr * parameter.grad\n",
    "                parameter.grad = torch.zeros_like(parameter)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Того же самого результата можно добиться, используя встроенные в ```pytorch``` оптимизаторы и функции для обновления и обнуления градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виды возможных оптимизаторов:<br>\n",
    "```optim.SGD``` - стохастический градиентный градиентный спуск (на самом деле, ему можно подавать батч любого размера, так что SGD - градиентный спуск по минибатчам)<br>\n",
    "```optim.Adam```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm)<br>\n",
    "```optim.RMSprop```  [ссылка на курсеру](https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop) <br>\n",
    "<br>\n",
    "[Полный список оптимизаторов](https://pytorch.org/docs/stable/optim.html])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы инициализировать оптимизатор, в него надо передать параметры обучаемой модели ```net.parameters()``` и начальную скорость обучения (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 20.01854820549488\n",
      "Loss on epoch 10 - 9.338072014972568\n",
      "Loss on epoch 20 - 7.126234911382198\n",
      "Loss on epoch 30 - 4.09489004034549\n",
      "Loss on epoch 40 - 2.2879429147578776\n",
      "Loss on epoch 50 - 1.5102364635095\n",
      "Loss on epoch 60 - 1.1316260362509638\n",
      "Loss on epoch 70 - 0.908867614576593\n",
      "Loss on epoch 80 - 0.7674200985347852\n",
      "Loss on epoch 90 - 0.6678514525992796\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net2 = Net()\n",
    "\n",
    "# В нашем случае будем использовать обычный градиентный спуск по минибатчам, как и в прошлый раз\n",
    "optimizer = optim.SGD(net2.parameters(), lr = 0.1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(xm_train,ym_train,64,shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net2(data.float())\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    if epoch%10==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(data,target):\n",
    "    df = pd.DataFrame(dict(x=data[:,0], y=data[:,1], label=target))\n",
    "    colors = {0:'red', 1:'blue'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_labels(xm_test.numpy(),np.round(net2(xm_test.float()).detach()).reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть на датасете Fashion MNIST, содержащем 10 видов различной одежды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "ycat_train = to_categorical(y_train, num_classes=10)\n",
    "ycat_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0b9e86deb8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0],cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycat_train = torch.tensor(ycat_train)\n",
    "ycat_test = torch.tensor(ycat_test)\n",
    "\n",
    "x_train = torch.tensor(x_train).view(-1,28*28).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "x_test = torch.tensor(x_test).view(-1,28*28).float()\n",
    "y_test = torch.tensor(y_test).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000]) torch.Size([10000, 784]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель с ```nn.CrossEntropyLoss()```. На последнем слое не будем применять ```torch.softmax```, так как в ```nn.CrossEntropyLoss()``` эта функция уже применяется. <br>\n",
    "Но чтобы получить нормальные предсказания в виде вектора вероятностей быть той или иной цифрой и проверить точность предсказаний, применить ```torch.softmax``` все же придется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistSoftmaxNet(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MnistSoftmaxNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x.float()))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MnistSoftmaxNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 1/5 [00:02<00:08,  2.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:04<00:06,  2.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 1 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:06<00:04,  2.20s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 2 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:09<00:02,  2.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 3 - 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 4 - 0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net = MnistSoftmaxNet()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(x_train, y_train, 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        #running_loss += loss.item()\n",
    "            \n",
    "    if epoch%1==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили довольно высокую точность на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float(),-1),dim = -1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле, создавать модели можно и более простым способом. Но тогда теряется гибкость настройки модели и мы лишаемся возможности создавать свои слои:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(768, 128))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(128, 64))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(64, 10))\n",
    "\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1/5 [00:01<00:07,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0 - 325.4367370605469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:03<00:05,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 1 - 131.55807495117188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:05<00:03,  1.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 2 - 89.6699447631836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 3 - 66.72305297851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 4 - 51.70305633544922\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "    \n",
    "net = MnistSoftmaxNet().cuda()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in generate_minibatches(x_train.cuda(), y_train.cuda(), 64, shuffle = True):\n",
    "        \n",
    "        data, labels = batch\n",
    "        \n",
    "        # Прямое распространение\n",
    "        output = net(data)\n",
    "        \n",
    "        # Подсчет ошибки на батче\n",
    "        loss = criterion(output, labels.long()) \n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        \n",
    "        #Обновление весов\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Обнуление градиентов\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        running_loss += loss #вызов .item() переводит полученное число на cpu, что замедляет работу\n",
    "    if epoch%1==0:\n",
    "        print(f\"Loss on epoch {epoch} - {running_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(torch.argmax(torch.softmax(net(x_test.float().cuda()),-1),dim = -1).cpu(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели базовые понятия ```pytorch``` и научились строить модели полносвязных нейронных сетей используя встроенные функции."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
